{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# REINFORCE"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from network.PolicyNetwork import PolicyNetwork\n",
    "from train import train, test\n",
    "from agents_tictactoe.ReinforceAgent import ReinforceAgent\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from env.TicTacToeEnvironment import TicTacToeEnvironment\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "env = TicTacToeEnvironment()\n",
    "num_cells = env.board_size[0]*env.board_size[1]\n",
    "num_hidden_units = 64\n",
    "num_layers = 1\n",
    "dropout_probability = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train\n",
    "The TRAINING procedure has already finished. Only run the train part when you want train again."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "net = PolicyNetwork(num_cells, num_hidden_units, num_layers, dropout_probability, num_cells).to(device)\n",
    "\n",
    "agent_a = ReinforceAgent(env, net, lr = 0.002, weight_decay=0.01)\n",
    "agent_b = ReinforceAgent(env)\n",
    "writer=SummaryWriter('runs/tictactoe_8k/reinforce/random')\n",
    "train(env, agent_a, agent_b, episodes=80000, log_interval=1000, writer=writer)\n",
    "test(env, agent_a, agent_b)\n",
    "torch.save(net, 'models/tictactoe/reinforce/random_8k_0.002.pth')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "net_a = PolicyNetwork(num_cells, num_hidden_units, num_layers, dropout_probability, num_cells)\n",
    "net_b = PolicyNetwork(num_cells, num_hidden_units, num_layers, dropout_probability, num_cells)\n",
    "agent_a = ReinforceAgent(env, net_a, lr = 0.002, weight_decay=0.01)\n",
    "agent_b = ReinforceAgent(env, net_b, lr = 0.002, weight_decay=0.01)\n",
    "\n",
    "writer=SummaryWriter('runs/tictactoe_8k/reinforce/agents')\n",
    "train(env, agent_a, agent_b, episodes=80000, log_interval=1000, writer=writer)\n",
    "test(env, agent_a, agent_b)\n",
    "torch.save(net_a, 'models/tictactoe/reinforce/agents_8k_0.002.pth')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TEST"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_with_random(policy_net, draw_board: bool = False, episodes: int = 10000):\n",
    "    env = TicTacToeEnvironment()\n",
    "    agent_a = ReinforceAgent(env, policy_net)\n",
    "    agent_b = ReinforceAgent(env)\n",
    "    test(env, agent_a, agent_b, draw_board=draw_board, episodes=episodes)\n",
    "\n",
    "\n",
    "def train_with_agents(policy_net_1, policy_net_2, draw_board: bool = True):\n",
    "    env = TicTacToeEnvironment()\n",
    "    agent_a = ReinforceAgent(env, policy_net_1)\n",
    "    agent_b = ReinforceAgent(env, policy_net_2)\n",
    "    test(env, agent_a, agent_b, draw_board=draw_board, episodes=1)\n",
    "\n",
    "\n",
    "## Test the agent trained with random sampling\n",
    "# Load the networks\n",
    "policy_net_random = torch.load('models/tictactoe/reinforce/random_40k_0.001.pth')\n",
    "# Test with random sampling\n",
    "train_with_random(policy_net_random)\n",
    "## Test the agent trained with dual agents\n",
    "# Load the networks\n",
    "policy_net_agents = torch.load('models/tictactoe/reinforce/agents_40k_0.001.pth')\n",
    "# Test with random sampling\n",
    "train_with_random(policy_net_agents)\n",
    "## Random Sampling vs. Dual Agents\n",
    "train_with_agents(policy_net_random, policy_net_agents)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
