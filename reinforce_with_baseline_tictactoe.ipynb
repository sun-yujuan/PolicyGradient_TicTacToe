{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# REINFORCE with Baseline (Separate Network)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from network.PolicyNetwork import PolicyNetwork\n",
    "from network.StateValueNetwork import StateValueNetwork\n",
    "from train import train, test\n",
    "from agents_tictactoe.ReinforceBaselineAgent import ReinforceBaselineAgent\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from env.TicTacToeEnvironment import TicTacToeEnvironment\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "env = TicTacToeEnvironment()\n",
    "num_cells = env.board_size[0] * env.board_size[1]\n",
    "num_hidden_units = 64\n",
    "num_layers = 1\n",
    "dropout_probability = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## TRAIN\n",
    "The TRAINING procedure has already finished. Only run the train part when you want train again."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train with Random Sampling"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "policy_net = PolicyNetwork(num_cells, num_hidden_units, num_layers, dropout_probability, num_cells).to(device)\n",
    "value_net = StateValueNetwork(num_cells, num_hidden_units, num_layers, dropout_probability).to(device)\n",
    "\n",
    "agent_a = ReinforceBaselineAgent(env, policy_net, value_net, lr=0.002, weight_decay=0.01)\n",
    "agent_b = ReinforceBaselineAgent(env)\n",
    "\n",
    "writer = SummaryWriter('runs/tictactoe_8k/reinforce_with_baseline/random')\n",
    "train(env, agent_a, agent_b, episodes=80000, log_interval=1000, writer=writer)\n",
    "test(env, agent_a, agent_b)\n",
    "torch.save(policy_net, 'models/tictactoe/reinforce_with_baseline/policy_random_8k_0.002.pth')\n",
    "torch.save(value_net, 'models/tictactoe/reinforce_with_baseline/value_random_8k_0.002.pth')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train with Dual Agents"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "policy_net_a = PolicyNetwork(num_cells, num_hidden_units, num_layers, dropout_probability, num_cells).to(device)\n",
    "value_net_a = StateValueNetwork(num_cells, num_hidden_units, num_layers, dropout_probability).to(device)\n",
    "\n",
    "policy_net_b = PolicyNetwork(num_cells, num_hidden_units, num_layers, dropout_probability, num_cells).to(device)\n",
    "value_net_b = StateValueNetwork(num_cells, num_hidden_units, num_layers, dropout_probability).to(device)\n",
    "\n",
    "agent_a = ReinforceBaselineAgent(env, policy_net_a, value_net_a, lr=0.002, weight_decay=0.01)\n",
    "agent_b = ReinforceBaselineAgent(env, policy_net_b, value_net_b, lr=0.002, weight_decay=0.01)\n",
    "\n",
    "writer = SummaryWriter('runs/tictactoe_8k/reinforce_with_baseline/agents')\n",
    "train(env, agent_a, agent_b, episodes=80000, log_interval=1000, writer=writer)\n",
    "test(env, agent_a, agent_b)\n",
    "torch.save(policy_net_a, 'models/tictactoe/reinforce_with_baseline/policy_agents_8k_0.002.pth')\n",
    "torch.save(value_net_a, 'models/tictactoe/reinforce_with_baseline/value_agents_8k_0.002.pth')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TEST"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_with_random(policy_net, value_net, draw_board: bool = False, episodes: int = 10000):\n",
    "    env = TicTacToeEnvironment()\n",
    "    agent_a = ReinforceBaselineAgent(env, policy_net, value_net)\n",
    "    agent_b = ReinforceBaselineAgent(env)\n",
    "    test(env, agent_a, agent_b, draw_board=draw_board, episodes=episodes)\n",
    "\n",
    "\n",
    "def train_with_agents(policy_net_1, value_net_1, policy_net_2, value_net_2, draw_board: bool = True):\n",
    "    env = TicTacToeEnvironment()\n",
    "    agent_a = ReinforceBaselineAgent(env, policy_net_1, value_net_1)\n",
    "    agent_b = ReinforceBaselineAgent(env, policy_net_2, value_net_2)\n",
    "    test(env, agent_a, agent_b, draw_board=draw_board, episodes=10)\n",
    "\n",
    "# Load the networks\n",
    "policy_net_random = torch.load('models/tictactoe/reinforce_with_baseline/policy_random_40k_0.001.pth')\n",
    "value_net_random = torch.load('models/tictactoe/reinforce_with_baseline/value_random_40k_0.001.pth')\n",
    "\n",
    "# Test with random sampling\n",
    "train_with_random(policy_net_random, value_net_random)\n",
    "\n",
    "# Load the networks\n",
    "policy_net_agents = torch.load('models/tictactoe/reinforce_with_baseline/policy_agents_40k_0.001.pth')\n",
    "value_net_agents = torch.load('models/tictactoe/reinforce_with_baseline/value_agents_40k_0.001.pth')\n",
    "\n",
    "# Load the networks\n",
    "policy_net_agents = torch.load('models/tictactoe/reinforce_with_baseline/policy_agents_40k_0.001.pth')\n",
    "value_net_agents = torch.load('models/tictactoe/reinforce_with_baseline/value_agents_40k_0.001.pth')\n",
    "\n",
    "# Test with random sampling\n",
    "train_with_random(policy_net_agents, value_net_agents)\n",
    "\n",
    "# Random Sampling vs. Dual Agents\n",
    "train_with_agents(policy_net_random, value_net_random, policy_net_agents, value_net_agents)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
